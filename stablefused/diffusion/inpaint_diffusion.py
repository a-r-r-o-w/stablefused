import numpy as np
import torch
import torch.nn.functional as F

from PIL import Image
from diffusers import AutoencoderKL
from tqdm.auto import tqdm
from transformers import CLIPTextModel, CLIPTokenizer
from typing import Any, List, Optional, Tuple, Union

from stablefused.diffusion import BaseDiffusion
from stablefused.typing import (
    ImageType,
    OutputType,
    PromptType,
    SchedulerType,
    UNetType,
)


class InpaintDiffusion(BaseDiffusion):
    def __init__(
        self,
        model_id: str = None,
        tokenizer: CLIPTokenizer = None,
        text_encoder: CLIPTextModel = None,
        vae: AutoencoderKL = None,
        unet: UNetType = None,
        scheduler: SchedulerType = None,
        torch_dtype: torch.dtype = torch.float32,
        device="cuda",
        *args,
        **kwargs,
    ) -> None:
        super().__init__(
            model_id=model_id,
            tokenizer=tokenizer,
            text_encoder=text_encoder,
            vae=vae,
            unet=unet,
            scheduler=scheduler,
            torch_dtype=torch_dtype,
            device=device,
            *args,
            **kwargs,
        )

    def embedding_to_latent(
        self,
        embedding: torch.FloatTensor,
        num_inference_steps: int,
        start_step: int,
        guidance_scale: float,
        guidance_rescale: float,
        latent: torch.FloatTensor,
        mask: torch.FloatTensor,
        masked_image_latent: torch.FloatTensor,
        return_latent_history: bool = False,
    ) -> Union[torch.FloatTensor, List[torch.FloatTensor]]:
        """
        Generate latent by conditioning on prompt embedding and input latent using diffusion.

        Parameters
        ----------
        embedding: torch.FloatTensor
            Embedding of text prompt.
        num_inference_steps: int
            Number of diffusion steps to run.
        start_step: int
            Step to start diffusion from. The higher the value, the more similar the generated
            image will be to the input image.
        guidance_scale: float
            Guidance scale encourages the model to generate images following the prompt
            closely, albeit at the cost of image quality.
        guidance_rescale: float
            Guidance rescale from [Common Diffusion Noise Schedules and Sample Steps are
            Flawed](https://arxiv.org/pdf/2305.08891.pdf).
        latent: torch.FloatTensor
            Latent to start diffusion from.
        mask: torch.FloatTensor
            Mask to condition on. Values below 0.5 are treated as 0 and values above 0.5
            are treated as 1. Values that are 1 (white) will be inpainted and values that
            are 0 (black) will be preserved.
        masked_image_latent: torch.FloatTensor
            Latent of masked image.
        return_latent_history: bool
            Whether to return latent history. If True, return list of all latents
            generated during diffusion steps.

        Returns
        -------
        Union[torch.FloatTensor, List[torch.FloatTensor]]
            Latent generated by diffusion. If return_latent_history is True, return list of
            all latents generated during diffusion steps.
        """

        # Set number of inference steps
        self.scheduler.set_timesteps(num_inference_steps)

        # Add noise to latent based on start step
        start_timestep = (
            self.scheduler.timesteps[start_step].repeat(latent.shape[0]).long()
        )
        noise = self.random_tensor(latent.shape)
        latent = self.scheduler.add_noise(latent, noise, start_timestep)

        timesteps = self.scheduler.timesteps[start_step:]
        latent_history = [latent]

        # Concatenate mask and masked_image_latent as required by classifier free guidance
        mask = torch.cat([mask] * 2)
        masked_image_latent = torch.cat([masked_image_latent] * 2)

        # Diffusion inference loop
        for i, timestep in tqdm(list(enumerate(timesteps))):
            # Duplicate latent to avoid two forward passes to perform classifier free guidance
            latent_model_input = torch.cat([latent] * 2)
            latent_model_input = self.scheduler.scale_model_input(
                latent_model_input, timestep
            )

            # TODO: The image inpainting model "runwayml/stable-diffusion-inpainting" requires
            # 9 channels as input. The first 4 channels are the image latent, the next 1 channel
            # is the mask and the last 4 channels are the masked image latent. This assumes that
            # all models have the same number of input channels but that may not be the case.
            # Refactor this logic to be more generic.
            latent_model_input = torch.cat(
                [latent_model_input, mask, masked_image_latent], dim=1
            )

            # Predict noise
            noise_prediction = self.unet(
                latent_model_input,
                timestep,
                encoder_hidden_states=embedding,
                return_dict=False,
            )[0]

            # Perform classifier free guidance
            noise_prediction = self.classifier_free_guidance(
                noise_prediction, guidance_scale, guidance_rescale
            )

            # Update latent
            latent = self.scheduler.step(
                noise_prediction, timestep, latent, return_dict=False
            )[0]

            if return_latent_history:
                latent_history.append(latent)

        return torch.stack(latent_history) if return_latent_history else latent

    def _handle_preprocess_tensor(
        self,
        image: torch.Tensor,
        mask: torch.Tensor,
    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:
        """
        Helper function to preprocess tensors representing images. Note that
        this should only be used for images already preprocessed with relevant logic in
        the _handle_preprocess_pil and _handle_preprocess_numpy functions.
        """

        if image.ndim != 4 or mask.ndim != 4:
            raise ValueError(
                f"image and mask must have 4 dimensions, got {image.ndim} and {mask.ndim}"
            )
        if image.shape[0] != mask.shape[0]:
            raise ValueError(
                f"image and mask must have same batch size, got {image.shape[0]} and {mask.shape[0]}"
            )
        if image.shape[1] != 3 or mask.shape[1] != 1:
            raise ValueError(
                f"image must have 3 channels and mask must have 1 channel, got {image.shape[1]} and {mask.shape[1]}"
            )
        if image.shape[2] != mask.shape[2] or image.shape[3] != mask.shape[3]:
            raise ValueError(
                f"image and mask must have same height and width, got {image.shape[2:]} and {mask.shape[2:]}"
            )
        if image.min() < -1 or image.max() > 1:
            raise ValueError(
                f"image must have values between -1 and 1, got {image.min()} and {image.max()}"
            )
        if mask.min() < 0 or mask.max() > 1:
            raise ValueError(
                f"mask must have values between 0 and 1, got {mask.min()} and {mask.max()}"
            )

        mask[mask < 0.5] = 0
        mask[mask >= 0.5] = 1

        image.to(device=self.device, dtype=self.torch_dtype)
        mask.to(device=self.device, dtype=self.torch_dtype)

        masked_image = image * (1 - mask)

        return image, mask, masked_image

    def _handle_preprocess_numpy(
        self,
        image: np.ndarray,
        mask: np.ndarray,
        image_height: int,
        image_width: int,
    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:
        """
        Helper function to preprocess numpy arrays representing images. Note that
        this should only be used for images already preprocessed with relevant logic in
        the _handle_preprocess_pil function.
        """

        if image.ndim == 3:
            image = np.expand_dims(image, axis=0)
        if mask.ndim == 3:
            mask = np.expand_dims(mask, axis=0)
        if image.ndim != 4 or mask.ndim != 4:
            raise ValueError(
                f"image and mask must have 4 dimensions, got {image.ndim} and {mask.ndim}"
            )

        image = (image / 255.0) * 2 - 1
        mask = mask / 255.0

        image = image.transpose(0, 3, 1, 2)
        image = torch.from_numpy(image).to(device=self.device, dtype=self.torch_dtype)

        mask = mask.transpose(0, 3, 1, 2)
        mask = torch.from_numpy(mask).to(device=self.device, dtype=self.torch_dtype)

        return self._handle_preprocess_tensor(image, mask)

    def _handle_preprocess_pil(
        self,
        image: List[Image.Image],
        mask: List[Image.Image],
        image_height: int,
        image_width: int,
    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:
        """Helper function to preprocess PIL images."""
        image = [
            i.resize((image_height, image_width), resample=Image.LANCZOS).convert("RGB")
            for i in image
        ]
        mask = [
            i.resize((image_height, image_width), resample=Image.LANCZOS).convert("L")
            for i in mask
        ]

        image = np.array(image)
        mask = np.array(mask).reshape(-1, image_height, image_width, 1)

        return self._handle_preprocess_numpy(image, mask, image_height, image_width)

    def preprocess_image_and_mask(
        self,
        image: ImageType,
        mask: ImageType,
        image_height: int,
        image_width: int,
    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:
        """
        Preprocess image and mask to be used for inference.

        Parameters
        ----------
        image: ImageType
            Input image to condition on.
        mask: ImageType
            Input mask to condition on. Must have same height and width as image.
            Must have 1 channel. Must have values between 0 and 1. Values below 0.5
            are treated as 0 and values above 0.5 are treated as 1. Values that are
            1 (white) will be inpainted and values that are 0 (black) will be
            preserved.
        image_height: int
            Height of image to generate. If height of provided image and image_height
            do not match, image will be resized to image_height using PIL Lanczos method.
        image_width: int
            Width of image to generate. If width of provided image and image_width
            do not match, image will be resized to image_width using PIL Lanczos method.

        Returns
        -------
        Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]
            Tuple of image, mask and masked_image.
        """
        if image is None or mask is None:
            raise ValueError("image and mask cannot be None")
        if type(image) != type(mask):
            raise TypeError(
                f"image and mask must be of same type, got {type(image)} and {type(mask)}"
            )
        if isinstance(image, Image.Image):
            image = [image]
            mask = [mask]

        if isinstance(image, torch.Tensor):
            image, mask, masked_image = self._handle_preprocess_tensor(image, mask)
        elif isinstance(image, np.ndarray):
            image, mask, masked_image = self._handle_preprocess_numpy(
                image, mask, image_height, image_width
            )
        else:
            image, mask, masked_image = self._handle_preprocess_pil(
                image, mask, image_height, image_width
            )

        return image, mask, masked_image

    @torch.no_grad()
    def __call__(
        self,
        prompt: PromptType,
        image: ImageType,
        mask: ImageType,
        image_height: int = 512,
        image_width: int = 512,
        num_inference_steps: int = 0,
        start_step: int = 0,
        guidance_scale: float = 7.5,
        guidance_rescale: float = 0.7,
        negative_prompt: Optional[PromptType] = None,
        output_type: str = "pil",
        return_latent_history: bool = False,
    ) -> OutputType:
        """
        Run inference by conditioning on input image, mask and prompt.

        Parameters
        ----------
        prompt: PromptType
            Text prompt to condition on.
        image: ImageType
            Input image to condition on.
        mask: ImageType
            Input mask to condition on. Must have same height and width as image.
            Must have 1 channel. Must have values between 0 and 1. Values below 0.5
            are treated as 0 and values above 0.5 are treated as 1. Values that are
            1 (white) will be inpainted and values that are 0 (black) will be
            preserved.
        image_height: int
            Height of image to generate. If height of provided image and image_height
            do not match, image will be resized to image_height using PIL Lanczos method.
        image_width: int
            Width of image to generate. If width of provided image and image_width
            do not match, image will be resized to image_width using PIL Lanczos method.
        num_inference_steps: int
            Number of diffusion steps to run.
        start_step: int
            Step to start diffusion from. The higher the value, the more similar the generated
            image will be to the input image.
        guidance_scale: float
            Guidance scale encourages the model to generate images following the prompt
            closely, albeit at the cost of image quality.
        guidance_rescale: float
            Guidance rescale from [Common Diffusion Noise Schedules and Sample Steps are
            Flawed](https://arxiv.org/pdf/2305.08891.pdf).
        negative_prompt: Optional[PromptType]
            Negative text prompt to uncondition on.
        output_type: str
            Type of output to return. One of ["latent", "pil", "pt", "np"].
        return_latent_history: bool
            Whether to return the latent history. If True, return list of all latents
            generated during diffusion steps.

        Returns
        -------
        OutputType
            Generated output based on output_type.
        """

        # Validate input
        self.validate_input(
            prompt=prompt,
            negative_prompt=negative_prompt,
            image_height=image_height,
            image_width=image_width,
            start_step=start_step,
            num_inference_steps=num_inference_steps,
        )

        # Preprocess image and mask
        image, mask, masked_image = self.preprocess_image_and_mask(
            image=image,
            mask=mask,
            image_height=image_height,
            image_width=image_width,
        )

        # Generate embedding to condition on prompt and uncondition on negative prompt
        embedding = self.prompt_to_embedding(
            prompt=prompt,
            negative_prompt=negative_prompt,
        )

        # Generate latent from input image and masked_image. Mask is down/up-scaled to match latent shape
        image_latent = self.image_to_latent(image)
        mask = F.interpolate(mask, size=(image_latent.shape[2], image_latent.shape[3]))
        masked_image_latent = self.image_to_latent(masked_image)

        # Run inference
        latent = self.embedding_to_latent(
            embedding=embedding,
            num_inference_steps=num_inference_steps,
            start_step=start_step,
            guidance_scale=guidance_scale,
            guidance_rescale=guidance_rescale,
            latent=image_latent,
            mask=mask,
            masked_image_latent=masked_image_latent,
            return_latent_history=return_latent_history,
        )

        return self.resolve_output(
            latent=latent,
            output_type=output_type,
            return_latent_history=return_latent_history,
        )

    generate = __call__
