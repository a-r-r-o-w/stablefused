import numpy as np
import torch

from PIL import Image
from diffusers import AutoencoderKL
from tqdm.auto import tqdm
from transformers import CLIPTextModel, CLIPTokenizer
from typing import List, Optional, Union

from stablefused.diffusion import BaseDiffusion
from stablefused.typing import PromptType, OutputType, SchedulerType, UNetType


class TextToVideoDiffusion(BaseDiffusion):
    def __init__(
        self,
        model_id: str = None,
        tokenizer: CLIPTokenizer = None,
        text_encoder: CLIPTextModel = None,
        vae: AutoencoderKL = None,
        unet: UNetType = None,
        scheduler: SchedulerType = None,
        torch_dtype: torch.dtype = torch.float32,
        device="cuda",
        *args,
        **kwargs
    ) -> None:
        super().__init__(
            model_id=model_id,
            tokenizer=tokenizer,
            text_encoder=text_encoder,
            vae=vae,
            unet=unet,
            scheduler=scheduler,
            torch_dtype=torch_dtype,
            device=device,
            *args,
            **kwargs
        )

    def embedding_to_latent(
        self,
        embedding: torch.FloatTensor,
        video_height: int,
        video_width: int,
        video_frames: int,
        num_inference_steps: int,
        guidance_scale: float,
        guidance_rescale: float,
        latent: Optional[torch.FloatTensor] = None,
    ) -> Union[torch.FloatTensor, List[torch.FloatTensor]]:
        """
        Generate latent by conditioning on prompt embedding using diffusion.

        Parameters
        ----------
        embedding: torch.FloatTensor
            Embedding of text prompt.
        video_height: int
            Height of video to generate.
        video_width: int
            Width of video to generate.
        video_frames: int
            Number of frames to generate in video.
        num_inference_steps: int
            Number of diffusion steps to run.
        guidance_scale: float
            Guidance scale encourages the model to generate images following the prompt
            closely, albeit at the cost of image quality.
        guidance_rescale: float
            Guidance rescale from [Common Diffusion Noise Schedules and Sample Steps are
            Flawed](https://arxiv.org/pdf/2305.08891.pdf).
        latent: Optional[torch.FloatTensor]
            Latent to start from. If None, generate latent from noise.

        Returns
        -------
        Union[torch.FloatTensor, List[torch.FloatTensor]]
            Latent generated by diffusion.
        """

        # Generate latent from noise if not provided
        if latent is None:
            shape = (
                embedding.shape[0] // 2,
                self.unet.config.in_channels,
                video_frames,
                video_height // self.vae_scale_factor,
                video_width // self.vae_scale_factor,
            )
            latent = self.random_tensor(shape)

        # Set number of inference steps
        self.scheduler.set_timesteps(num_inference_steps)
        timesteps = self.scheduler.timesteps

        # Scale the latent noise by the standard deviation required by the scheduler
        latent = latent * self.scheduler.init_noise_sigma

        # Diffusion inference loop
        for i, timestep in tqdm(list(enumerate(timesteps))):
            # Duplicate latent to avoid two forward passes to perform classifier free guidance
            latent_model_input = torch.cat([latent] * 2)
            latent_model_input = self.scheduler.scale_model_input(
                latent_model_input, timestep
            )

            # Predict noise
            noise_prediction = self.unet(
                latent_model_input,
                timestep,
                encoder_hidden_states=embedding,
                return_dict=False,
            )[0]

            # Perform classifier free guidance
            noise_prediction = self.classifier_free_guidance(
                noise_prediction, guidance_scale, guidance_rescale
            )

            # Update latent
            latent = self.scheduler.step(
                noise_prediction, timestep, latent, return_dict=False
            )[0]

        return latent

    def resolve_output(
        self,
        latent: torch.FloatTensor,
        output_type: str,
        decode_batch_size: int,
    ) -> OutputType:
        """
        Resolve output type from latent.

        Parameters
        ----------
        latent: torch.FloatTensor
            Latent to resolve output from.
        output_type: str
            Output type to resolve. Must be one of [`latent`, `pt`, `np`, `pil`].
        decode_batch_size: int
            Batch size to use when decoding latent to image.

        Returns
        -------
        OutputType
            The resolved output based on the provided latent vector and options.
        """

        if output_type not in ["latent", "pt", "np", "pil"]:
            raise ValueError(
                "`output_type` must be one of [`latent`, `pt`, `np`, `pil`]"
            )

        if output_type == "latent":
            return latent

        # B, C, F, H, W => B, F, C, H, W
        latent = latent.permute(0, 2, 1, 3, 4)
        video = []

        for i in tqdm(range(latent.shape[0])):
            batched_output = []
            for j in tqdm(range(0, latent.shape[1], decode_batch_size)):
                current_latent = latent[i, j : j + decode_batch_size]
                batched_output.extend(self.latent_to_image(current_latent, output_type))
            video.append(batched_output)

        if output_type == "pt":
            video = torch.stack(video)
        elif output_type == "np":
            video = np.stack(video)

        return video

    @torch.no_grad()
    def __call__(
        self,
        prompt: PromptType,
        video_height: int = 512,
        video_width: int = 512,
        video_frames: int = 24,
        num_inference_steps: int = 50,
        guidance_scale: float = 7.5,
        guidance_rescale: float = 0.7,
        negative_prompt: Optional[PromptType] = None,
        latent: Optional[torch.FloatTensor] = None,
        output_type: str = "pil",
        decode_batch_size: int = 4,
    ) -> OutputType:
        """
        Run inference by conditioning on text prompt.

        Parameters
        ----------
        prompt: PromptType
            Text prompt to condition on.
        video_height: int
            Height of video to generate.
        video_width: int
            Width of video to generate.
        video_frames: int
            Number of frames to generate in video.
        num_inference_steps: int
            Number of diffusion steps to run.
        guidance_scale: float
            Guidance scale encourages the model to generate images following the prompt
            closely, albeit at the cost of image quality.
        guidance_rescale: float
            Guidance rescale from [Common Diffusion Noise Schedules and Sample Steps are
            Flawed](https://arxiv.org/pdf/2305.08891.pdf).
        negative_prompt: Optional[PromptType]
            Negative text prompt to uncondition on.
        latent: Optional[torch.FloatTensor]
            Latent to start from. If None, latent is generated from noise.
        output_type: str
            Type of output to return. One of ["latent", "pil", "pt", "np"].
        decode_batch_size: int
            Batch size to use when decoding latent to image.

        Returns
        -------
        OutputType
            Generated output based on output_type.
        """

        # Validate input
        self.validate_input(
            prompt=prompt,
            negative_prompt=negative_prompt,
            image_height=video_height,
            image_width=video_width,
            num_inference_steps=num_inference_steps,
        )

        # Generate embedding to condition on prompt and uncondition on negative prompt
        embedding = self.prompt_to_embedding(
            prompt=prompt,
            negative_prompt=negative_prompt,
        )

        # Run inference
        latent = self.embedding_to_latent(
            embedding=embedding,
            video_height=video_height,
            video_width=video_width,
            video_frames=video_frames,
            num_inference_steps=num_inference_steps,
            guidance_scale=guidance_scale,
            guidance_rescale=guidance_rescale,
            latent=latent,
        )

        return self.resolve_output(
            latent=latent,
            output_type=output_type,
            decode_batch_size=decode_batch_size,
        )

    generate = __call__
