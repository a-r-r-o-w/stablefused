import numpy as np
import torch

from PIL import Image
from diffusers import AutoencoderKL
from tqdm.auto import tqdm
from transformers import CLIPTextModel, CLIPTokenizer
from typing import List, Optional, Union

from stablefused.diffusion import BaseDiffusion
from stablefused.typing import PromptType, OutputType, SchedulerType, UNetType


class ImageToImageDiffusion(BaseDiffusion):
    def __init__(
        self,
        model_id: str = None,
        tokenizer: CLIPTokenizer = None,
        text_encoder: CLIPTextModel = None,
        vae: AutoencoderKL = None,
        unet: UNetType = None,
        scheduler: SchedulerType = None,
        torch_dtype: torch.dtype = torch.float32,
        device="cuda",
        *args,
        **kwargs
    ) -> None:
        super().__init__(
            model_id=model_id,
            tokenizer=tokenizer,
            text_encoder=text_encoder,
            vae=vae,
            unet=unet,
            scheduler=scheduler,
            torch_dtype=torch_dtype,
            device=device,
            *args,
            **kwargs
        )

    def embedding_to_latent(
        self,
        embedding: torch.FloatTensor,
        num_inference_steps: int,
        start_step: int,
        guidance_scale: float,
        guidance_rescale: float,
        latent: torch.FloatTensor,
        return_latent_history: bool = False,
    ) -> Union[torch.FloatTensor, List[torch.FloatTensor]]:
        """
        Generate latent by conditioning on prompt embedding and input image using diffusion.

        Parameters
        ----------
        embedding: torch.FloatTensor
            Embedding of text prompt.
        num_inference_steps: int
            Number of diffusion steps to run.
        start_step: int
            Step to start diffusion from. The higher the value, the more similar the generated
            image will be to the input image.
        guidance_scale: float
            Guidance scale encourages the model to generate images following the prompt
            closely, albeit at the cost of image quality.
        guidance_rescale: float
            Guidance rescale from [Common Diffusion Noise Schedules and Sample Steps are
            Flawed](https://arxiv.org/pdf/2305.08891.pdf).
        latent: torch.FloatTensor
            Latent to start diffusion from.
        return_latent_history: bool
            Whether to return the latent history. If True, return list of all latents
            generated during diffusion steps.

        Returns
        -------
        Union[torch.FloatTensor, List[torch.FloatTensor]]
            Latent generated by diffusion. If return_latent_history is True, return list of
            all latents generated during diffusion steps.
        """

        latent = latent.to(self.device)

        # Set number of inference steps
        self.scheduler.set_timesteps(num_inference_steps)

        # Add noise to latent based on start step
        start_timestep = (
            self.scheduler.timesteps[start_step].repeat(latent.shape[0]).long()
        )
        noise = self.random_tensor(latent.shape)
        latent = self.scheduler.add_noise(latent, noise, start_timestep)

        timesteps = self.scheduler.timesteps[start_step:]
        latent_history = [latent]

        # Diffusion inference loop
        for i, timestep in tqdm(list(enumerate(timesteps))):
            # Duplicate latent to avoid two forward passes to perform classifier free guidance
            latent_model_input = torch.cat([latent] * 2)
            latent_model_input = self.scheduler.scale_model_input(
                latent_model_input, timestep
            )

            # Predict noise
            noise_prediction = self.unet(
                latent_model_input,
                timestep,
                encoder_hidden_states=embedding,
                return_dict=False,
            )[0]

            # Perform classifier free guidance
            noise_prediction = self.classifier_free_guidance(
                noise_prediction, guidance_scale, guidance_rescale
            )

            # Update latent
            latent = self.scheduler.step(
                noise_prediction, timestep, latent, return_dict=False
            )[0]

            if return_latent_history:
                latent_history.append(latent)

        return torch.stack(latent_history) if return_latent_history else latent

    @torch.no_grad()
    def __call__(
        self,
        image: Image.Image,
        prompt: PromptType,
        num_inference_steps: int = 50,
        start_step: int = 0,
        guidance_scale: float = 7.5,
        guidance_rescale: float = 0.7,
        negative_prompt: Optional[PromptType] = None,
        output_type: str = "pil",
        return_latent_history: bool = False,
    ) -> OutputType:
        """
        Run inference by conditioning on input image and text prompt.

        Parameters
        ----------
        image: Image.Image
            Input image to condition on.
        prompt: PromptType
            Text prompt to condition on.
        num_inference_steps: int
            Number of diffusion steps to run.
        start_step: int
            Step to start diffusion from. The higher the value, the more similar the generated
            image will be to the input image.
        guidance_scale: float
            Guidance scale encourages the model to generate images following the prompt
            closely, albeit at the cost of image quality.
        guidance_rescale: float
            Guidance rescale from [Common Diffusion Noise Schedules and Sample Steps are
            Flawed](https://arxiv.org/pdf/2305.08891.pdf).
        negative_prompt: Optional[PromptType]
            Negative text prompt to uncondition on.
        output_type: str
            Type of output to return. One of ["latent", "pil", "pt", "np"].
        return_latent_history: bool
            Whether to return the latent history. If True, return list of all latents
            generated during diffusion steps.

        Returns
        -------
        OutputType
            Generated output based on output_type.
        """

        # Validate input
        self.validate_input(
            prompt=prompt,
            negative_prompt=negative_prompt,
            start_step=start_step,
            num_inference_steps=num_inference_steps,
        )

        # Generate embedding to condition on prompt and uncondition on negative prompt
        embedding = self.prompt_to_embedding(
            prompt=prompt,
            negative_prompt=negative_prompt,
        )

        # Generate latent from input image
        image_latent = self.image_to_latent(image)

        # Run inference
        latent = self.embedding_to_latent(
            embedding=embedding,
            num_inference_steps=num_inference_steps,
            start_step=start_step,
            guidance_scale=guidance_scale,
            guidance_rescale=guidance_rescale,
            latent=image_latent,
            return_latent_history=return_latent_history,
        )

        return self.resolve_output(
            latent=latent,
            output_type=output_type,
            return_latent_history=return_latent_history,
        )

    generate = __call__
