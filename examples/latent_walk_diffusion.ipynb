{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Walk Diffusion\n",
    "\n",
    "In this notebook, we will take a look at latent walking in latent spaces. Generative models, like the ones used in Stable Diffusion, learn a latent representation of the world. A latent representation is a low-dimensional vector space embedding of the world. In the case of SD, this latent representation is learnt by training on text-image pairs. This representation is used to generate samples given a prompt and a random noise vector. The model tries to predict and remove noise from the random noise vector, while also aligning it the vector to the prompt. This results in some interesting properties of the latent space. In this notebook, we will explore these properties.\n",
    "\n",
    "Stable Diffusion models (atleast, the models used here) learn two latent representations - one of the NLP space for prompts, and one of the image space. These latent representations are continuous. If we choose two vectors in the latent space to sample from, we get two different/similar images depending on how different the chosen vectors are. This is the basis of latent walking. We can choose two vectors in the latent space, and sample from the latent path between them. This results in a smooth transition between the two images."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install and Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install stablefused ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from IPython.display import Video, display\n",
    "from PIL import Image\n",
    "from stablefused import LatentWalkDiffusion, TextToImageDiffusion\n",
    "from stablefused.utils import image_grid, pil_to_video"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize model and parameters\n",
    "\n",
    "We use RunwayML's Stable Diffusion 1.5 checkpoint and initialize our Latent-Walk and Text-to-Image Diffusion models. Play around with different prompts and parameters, and see what you get! You can comment out the parts that use seeds to generate random images each time you run the notebook.\n",
    "\n",
    "We use the following mechanism to trade-off speed for reduced memory footprint. It allows us to work with bigger images and larger batch sizes with about just 6GB of GPU memory.\n",
    "- U-Net Attention Slicing: Allows the internal U-Net model to perform computations for attention heads sequentially, rather than in parallel.\n",
    "- VAE Slicing: Allow tensor slicing for VAE decode step. This will cause the vae to split the input tensor to compute decoding in multiple steps.\n",
    "- VAE Tiling: Allow tensor tiling for vae. This will cause the vae to split the input tensor into tiles to compute encoding/decoding in several steps.\n",
    "\n",
    "Also, notice how we are loading the same model twice. That should use twice the memory, right? Well, in most cases, users stick to using the same model checkpoints across different inference pipelines, and so it makes sense to share the internal models. StableFused maintains an internal model cache which allows all internal models to be shared, in order to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "lw_model = LatentWalkDiffusion(model_id=model_id, torch_dtype=torch.float16)\n",
    "\n",
    "lw_model.enable_attention_slicing()\n",
    "lw_model.enable_slicing()\n",
    "lw_model.enable_tiling()\n",
    "\n",
    "t2i_model = TextToImageDiffusion(model_id=model_id, torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVIENw0bnSZ_"
   },
   "source": [
    "Prompt Credits: https://mspoweruser.com/best-stable-diffusion-prompts/#6_The_Robotic_Baroque_Battle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Large futuristic mechanical robot in the foreground of a baroque-style battle scene, photorealistic, high quality, 8k\"\n",
    "negative_prompt = \"cartoon, unrealistic, blur, boring background, deformed, disfigured, low resolution, unattractive\"\n",
    "num_images = 4\n",
    "seed = 44\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There seems to be a bug in stablefused which requires to be reviewed. The\n",
    "# bug causes images created using latent walk to be very noisy, or plain white,\n",
    "# deformed, etc.\n",
    "# This is why instead of being able to use an actual image to generate latents,\n",
    "# we need to make it ourselves. In the future, this notebook will be updated\n",
    "# to allow latent walking for user-chosen images\n",
    "\n",
    "# filename = \"the-robotic-baroque-battle.png\"\n",
    "# start_image = [Image.open(filename)] * num_images\n",
    "\n",
    "# # This step is only required when loading model with torch.float16 dtype\n",
    "# start_image = np.array(start_image, dtype=np.float16)\n",
    "\n",
    "# latent = lw_model.image_to_latent(start_image)\n",
    "\n",
    "image_height = 512\n",
    "image_width = 512\n",
    "shape = (\n",
    "    1,\n",
    "    lw_model.unet.config.in_channels,\n",
    "    image_height // lw_model.vae_scale_factor,\n",
    "    image_width // lw_model.vae_scale_factor,\n",
    ")\n",
    "single_latent = lw_model.random_tensor(shape)\n",
    "latent = single_latent.repeat(num_images, 1, 1, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Walking to generate similar images\n",
    "\n",
    "Let's see what our base image for latent walking looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2i_model(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    num_inference_steps=20,\n",
    "    guidance_scale=10.0,\n",
    "    latent=single_latent,\n",
    ")[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent walk with diffusion around the latent space of our sampled latent vector. This results in generation of similar images. The similarity/difference can be controlled using the `strength` parameter (set between 0 and 1, defaults to 0.2). Lower strenght leads to similar images with subtle differences. Higher strength can cause completely new ideas to be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = lw_model(\n",
    "    prompt=[prompt] * num_images,\n",
    "    negative_prompt=[negative_prompt] * num_images,\n",
    "    latent=latent,\n",
    "    strength=0.25,\n",
    "    num_inference_steps=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_grid(images, rows=2, cols=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Videos with Latent Walking\n",
    "\n",
    "Here, we generate a video by walking the latent space of the model, using interpolation techniques to generate frames. An interpolation is just a weighted average of two embeddings calculated by some interpolation function. [Linear interpolation](https://en.wikipedia.org/wiki/Linear_interpolation) is used on the prompt embeddings and [Spherical Linear Interpolation](https://en.wikipedia.org/wiki/Slerp) is used on the latent embeddings, by default. You can change the interpolation method by passing `embedding_interpolation_type` or `latent_interpolation_type` parameter.\n",
    "\n",
    "Note that stablefused is a toy library in its infancy and is not optimized for speed, and does not support a lot of features. There are multiple bugs and issues that need to be addressed. Some things need to be implemented manually currently, but in the future, I hope to make the process easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt credits: ChatGPT\n",
    "story_prompt = [\n",
    "    \"A dog chasing a cat in a thrilling backyard scene, high quality and photorealistic\",\n",
    "    \"A determined dog in hot pursuit, with stunning realism, octane render\",\n",
    "    \"A thrilling chase, dog behind the cat, octane render, exceptional realism and quality\",\n",
    "    \"The exciting moment of a cat outmaneuvering a chasing dog, high-quality and photorealistic detail\",\n",
    "    \"A clever cat escaping a determined dog and soaring into space, rendered with octane render for stunning realism\",\n",
    "    \"The cat's escape into the cosmos, leaving the dog behind in a scene,high quality and photorealistic style\",\n",
    "]\n",
    "\n",
    "seed = 123456\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There seems to be a bug in stablefused which requires to be reviewed. The\n",
    "# bug causes images created using latent walk to be very noisy, or plain white,\n",
    "# deformed, etc.\n",
    "# This is why instead of being able to use an actual image to generate latents,\n",
    "# we need to make it ourselves. In the future, this notebook will be updated\n",
    "# to allow latent walking for user-chosen images\n",
    "\n",
    "# t2i_images = t2i_model(\n",
    "#     prompt = story_prompt,\n",
    "#     negative_prompt = [negative_prompt] * len(story_prompt),\n",
    "#     num_inference_steps = 20,\n",
    "#     guidance_scale = 12.0,\n",
    "# )\n",
    "\n",
    "image_height = 512\n",
    "image_width = 512\n",
    "shape = (\n",
    "    len(story_prompt),\n",
    "    lw_model.unet.config.in_channels,\n",
    "    image_height // lw_model.vae_scale_factor,\n",
    "    image_width // lw_model.vae_scale_factor,\n",
    ")\n",
    "latent = lw_model.random_tensor(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2i_images = t2i_model(\n",
    "    prompt=story_prompt,\n",
    "    num_inference_steps=20,\n",
    "    guidance_scale=15.0,\n",
    "    latent=latent,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_grid(t2i_images, rows=2, cols=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to the bug mentioned above, this step is not required.\n",
    "# We can directly use the latents we generated manually\n",
    "# np_t2i_images = np.array(t2i_images, dtype = np.float16)\n",
    "# t2i_latents = t2i_model.image_to_latent(np_t2i_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolation_steps = 24\n",
    "\n",
    "# Since stablefused does not support batch processing yet, we need\n",
    "# to do it manually. This notebook will be updated in the future\n",
    "# to support batching internally to handle a large number of images\n",
    "\n",
    "story_images = []\n",
    "for i in range(len(story_prompt) - 1):\n",
    "    current_prompt = story_prompt[i : i + 2]\n",
    "    current_latent = latent[i : i + 2]\n",
    "    imgs = lw_model.interpolate(\n",
    "        prompt=current_prompt,\n",
    "        negative_prompt=[negative_prompt] * len(current_prompt),\n",
    "        latent=current_latent,\n",
    "        num_inference_steps=20,\n",
    "        interpolation_steps=interpolation_steps,\n",
    "    )\n",
    "    story_images.extend(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"dog-chasing-cat-story.mp4\"\n",
    "pil_to_video(story_images, filename, fps=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Video(filename, embed=True))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
